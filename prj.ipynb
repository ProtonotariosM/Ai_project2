{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_it(text):\n",
    "    splt = text.split()\n",
    "    bad_chars = [';', ':',\" \", \"!\", \"*\", \"(\", \")\", '\\\"', \".\", \",\", \"/\", \">\", \"<\"]\n",
    "\n",
    "    for i in range(len(splt)):\n",
    "        for b in bad_chars:\n",
    "            if b in splt[i]:\n",
    "                splt[i] = splt[i].replace(b, '')\n",
    "        splt[i]= splt[i].lower()\n",
    "    \n",
    "    return splt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "from decimal import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def truncate_float(float_number, decimal_places):\n",
    "    multiplier = 10 ** decimal_places\n",
    "    return int(float_number * multiplier) / multiplier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CALCULATES THE IG OF EACH WORD\n",
    "def IG(w, total_pos, total_neg, path_pos, path_neg):\n",
    "    total_reviews= total_pos + total_neg\n",
    "    \n",
    "    \n",
    "     \n",
    "    files_positive = Path(path_pos).glob('*.txt')\n",
    "    files_negative = Path(path_neg).glob('*.txt')\n",
    "     \n",
    "    \n",
    "    \n",
    "    P1= (total_pos/total_reviews)  #probability of positive reviews\n",
    "    P0= (total_neg/total_reviews)  #probability of negative reviews\n",
    "    \n",
    "    entropy= -P1*math.log2(P1)-P0*math.log2(P0)\n",
    "    \n",
    "    \n",
    "    #Probabilities of C=pos and w=1 & C=neg and w=1\n",
    "    pos_counter=0\n",
    "    neg_counter=0 \n",
    "    total_counter=0\n",
    "    \n",
    "\n",
    "    \n",
    "    for f in files_positive:\n",
    "        sp= f.read_text().split()\n",
    "        if w.lower() in [x.lower() for x in sp]:\n",
    "            pos_counter+=1\n",
    "            total_counter+=1\n",
    "      \n",
    "    for f2 in files_negative:\n",
    "        sp2= f2.read_text().split()\n",
    "        if w.lower() in [x.lower() for x in sp2]:\n",
    "            neg_counter+=1\n",
    "            total_counter+=1\n",
    "    \n",
    "    \n",
    "    #Probability of w=1 and w=0 \n",
    "    P_w1 = total_counter/total_reviews\n",
    "    #print(\"Probability of \", w, \"=1: \", P_w1)\n",
    "    P_w0 = (total_reviews- total_counter) / total_reviews\n",
    "    #print(\"Probability of \", w, \"=0: \", P_w0)\n",
    "\n",
    "        \n",
    "   \n",
    "    #Probability of a C=pos and C=neg when w=1\n",
    "    if P_w1!=0:\n",
    "        P1_w1= pos_counter/total_counter \n",
    "        #print(\"probability positive reviews with \", w,\"=1: \", P1_w1)\n",
    "        P0_w1= neg_counter/total_counter\n",
    "        #print(\"probability negative reviews with \", w,\"=1: \", P0_w1)\n",
    "    else:\n",
    "        P1_w1=0\n",
    "        P0_w1=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Probability of a C=pos and C=neg when w=0\n",
    "    if P_w0!=0:\n",
    "        P1_w0 = (total_pos-pos_counter)/(total_reviews- total_counter)\n",
    "        #print(\"probability positive reviews with \", w,\"=0: \", P1_w0)\n",
    "        P0_w0 = (total_neg-neg_counter)/(total_reviews - total_counter)\n",
    "        #print(\"probability negative reviews with \", w,\"=0: \", P0_w0)\n",
    "    else:\n",
    "        P1_w0 = 0\n",
    "        P0_w0 = 0\n",
    "    \n",
    "    \n",
    "    entropy_w1= (0 if P1_w1==0 else -P1_w1*math.log2(P1_w1)) - (0 if P0_w1==0 else P0_w1*math.log2(P0_w1))\n",
    "    #print(\"Entropy of \", w, \"=1: \", entropy_w1)\n",
    "    entropy_w0= (0 if P1_w0==0 else -P1_w0*math.log2(P1_w0)) - (0 if P0_w0==0 else P0_w0*math.log2(P0_w0))\n",
    "    #print(\"Entropy of \", w, \"=0: \", entropy_w0)\n",
    "\n",
    "    \n",
    "    if entropy!=1.0:\n",
    "        ig= truncate_float(entropy, 6) - truncate_float((P_w1*entropy_w1 + P_w0*entropy_w0), 6)\n",
    "    else:\n",
    "        getcontext().prec = 6\n",
    "        ig= Decimal(1) - Decimal((P_w1*entropy_w1 + P_w0*entropy_w0))\n",
    "       \n",
    "    \n",
    "    \n",
    "    #print(\"IG of \",w , \"is: \", ig)\n",
    "    \n",
    "    return float(ig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FILTERS AN ARRAY OF WORDS BASED ON THEIR IG\n",
    "def IG_filter(words_filtered, total_pos, total_neg, path_pos, path_neg):\n",
    "    result=[]\n",
    "    \n",
    "    for w in words_filtered:\n",
    "        ig=IG(w, total_pos, total_neg, path_pos, path_neg)\n",
    "        if ig >=0.008:\n",
    "            result.append(w)\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_it(text):\n",
    "    splt = text.split()\n",
    "    bad_chars = [';', ':',\" \", \"!\", \"*\", \"(\", \")\", '\\\"', \".\", \",\", \"/\", \">\", \"<\"]\n",
    "\n",
    "    for i in range(len(splt)):\n",
    "        for b in bad_chars:\n",
    "            if b in splt[i]:\n",
    "                splt[i] = splt[i].replace(b, '')\n",
    "        splt[i]= splt[i].lower()\n",
    "    \n",
    "    return splt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class tokenizer():\n",
    "\n",
    "    #FINDING THE MOST USEFUL ATTRIBUTES (WORDS) IN ALL THE TRAINING DATA\n",
    "    #CONVERTING THE TRAINING DATA INTO VECTORS OF THE ATTRIBUTES WE CHOSE\n",
    "    def tokenize(self,path_pos, path_neg):\n",
    "        \n",
    "        files_positive = Path(path_pos).glob('*.txt')\n",
    "        files_negative = Path(path_neg).glob('*.txt')\n",
    "        \n",
    "        \n",
    "\n",
    "        total_pos=0\n",
    "        total_neg=0\n",
    "\n",
    "        text= \"\"\n",
    "        i=0\n",
    "        for file in files_positive:\n",
    "            #print(file.name)\n",
    "            \n",
    "            text= text+file.read_text()  # the file is opened and closed\n",
    "            total_pos += 1\n",
    "            \n",
    "            \n",
    "        for file2 in files_negative:\n",
    "            #print(file.name)\n",
    "            \n",
    "            total_neg+=1\n",
    "            text= text+file2.read_text()  \n",
    "            \n",
    "\n",
    "\n",
    "        total_reviews= total_pos+total_neg  \n",
    "\n",
    "        print(\"Total positive reviews: \",  total_pos)\n",
    "        print(\"Total negative reviews: \", total_neg)\n",
    "\n",
    "\n",
    "\n",
    "        #FILTERING THE REVIEWS TO KEEP THE USEFUL WORDS\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        splt= split_it(text)\n",
    "        Counters_found = Counter(splt)\n",
    "        most_occur = Counters_found.most_common(700)\n",
    "\n",
    "\n",
    "\n",
    "        ps = PorterStemmer()\n",
    "        words=[]\n",
    "        for i in range(len(most_occur)):\n",
    "            words.append(most_occur[i][0])\n",
    "                \n",
    "\n",
    "        bad_chars = [';', ':',\" \", \"!\", \"*\", \"(\", \")\", '\\\"', \".\", \",\", \"/\", \">\", \"<\"]\n",
    "        words_to_exclude = ['-', '--', ';', ':', \"!\", \"*\", \" \", \"(\", \")\", '\\\"', \".\", 'it\\'s', 'br', 'mr', 'there\\'s', 'your', 'wasn\\'t', 'Ms', 'were', 'how', 'get', 'will', 'also', 'been', 'some', 'into', 'because', 'about', 'out', 'me', 'up', 'down', 'my', 'mine', 'their', 'she', 'he\\'s', 'the','you', 'an', 'his', 'him', 'her', 'or', 'was', 'have', 'has', 'had', 'in', 'i', 'he', 'we', 'they', 'their', 'theirs', 'which','what', 'where', 'be', 'they', 'has', 'so',  'by', 'who', 'that','this', 'those', 'your', 'these', 'on', 'there', 'and', 'to', 'a', 'it', 'its', 'for', 'if', 'then', 'is', 'at', 'are', 'of', 'no', 'as', 'but', 'with', 'there']    \n",
    "\n",
    "        words_filtered=[]\n",
    "        for i, w in enumerate(words):\n",
    "            \n",
    "            \n",
    "            for b in bad_chars:\n",
    "                if b in words[i]:\n",
    "                    words[i]= words[i].replace(b,'')\n",
    "                    w=words[i]\n",
    "                \n",
    "\n",
    "            \n",
    "            \n",
    "            if w.lower() in words_to_exclude:\n",
    "                #print('removing ' +w)\n",
    "                #words.remove(w)\n",
    "                continue\n",
    "\n",
    "            words_filtered.append(words[i])        \n",
    "                \n",
    "\n",
    "\n",
    "        words_filtered= words_filtered[:len(words_filtered)-128]\n",
    "        \n",
    "        \n",
    "                \n",
    "        # c=1\n",
    "        # for i in  words_filtered:\n",
    "        #     print(c,'. '+i)\n",
    "        #     c=c+1\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        self.words_final= IG_filter(words_filtered, total_pos, total_neg, path_pos, path_neg)\n",
    "\n",
    "            \n",
    "            \n",
    "        files_positive2 =Path(path_pos).glob('*.txt')\n",
    "        files_negative2= Path(path_neg).glob('*.txt')\n",
    "            \n",
    "        #TOKENIZES EACH TEXT IN THE MOST USEFUL WORDS WE FOUND\n",
    "        self.vectors= np.zeros([total_reviews, len(self.words_final)+1], dtype=int)\n",
    "        i=0\n",
    "        for file in files_positive2:\n",
    "            i+=1\n",
    "            for j,w in enumerate(self.words_final):\n",
    "                if w.lower() in file.read_text().lower():\n",
    "                    self.vectors[i][j]=1\n",
    "            self.vectors[i][len(self.words_final)]=1  # initializing the last cell as 1 (category cell)\n",
    "    \n",
    "        i=0\n",
    "        for file2 in files_negative2:\n",
    "            i+=1\n",
    "            for j, w in enumerate(self.words_final):\n",
    "                if w.lower() in file2.read_text().lower():\n",
    "                    self.vectors[i][j]=1\n",
    "            #the last cell in the negative reviews is already 0 \n",
    "        \n",
    "        \n",
    "        vectors_df= pd.DataFrame(self.vectors, columns=self.words_final+ ['Category'])\n",
    "        display(vectors_df)\n",
    "        \n",
    "        return self.vectors\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_words_final(self):\n",
    "        return self.words_final\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def tokenize_new(self, path_pos, path_neg ):\n",
    "        \n",
    "        files_positive3 =Path(path_pos).glob('*.txt')\n",
    "        files_negative3= Path(path_neg).glob('*.txt')\n",
    "        \n",
    "        pos_count = len(os.listdir(path_pos))\n",
    "        neg_count = len(os.listdir(path_neg))\n",
    "        total_count =pos_count + neg_count\n",
    "        \n",
    "        new_vectors= np.zeros([total_count, len(self.words_final)+1], dtype=int)\n",
    "        \n",
    "        \n",
    "        i=0\n",
    "        for f in files_positive3:\n",
    "            i+=1\n",
    "            for j,w in enumerate(self.words_final):\n",
    "                if w.lower() in f.read_text().lower():\n",
    "                    new_vectors[i][j]=1\n",
    "            new_vectors[i][len(self.words_final)]=1  # initializing the last cell as 1 (category cell)\n",
    "    \n",
    "        i=0\n",
    "        for f2 in files_negative3:\n",
    "            i+=1\n",
    "            for j, w in enumerate(self.words_final):\n",
    "                if w.lower() in f2.read_text().lower():\n",
    "                    new_vectors[i][j]=1\n",
    "            #the last cell in the negative reviews is already 0 \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        vectors_df= pd.DataFrame(new_vectors, columns=self.words_final+ ['Category'])\n",
    "        display(vectors_df)\n",
    "        \n",
    "        return new_vectors, vectors_df\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Probability:\n",
    "    \n",
    "    def __init__(self, data, total_pos, total_neg):\n",
    "        self.data= data\n",
    "        self.total_pos = total_pos\n",
    "        self.total_neg = total_neg\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def prob_tables_training_data(self):\n",
    "        \n",
    "        #print(\"Starting to build the probability tables\")\n",
    "        self.pX_1= np.zeros([2,self.data.shape[1]], dtype=float)\n",
    "        self.pX_1[1][self.data.shape[1]-1] = 1 \n",
    "        self.pX_0 = np.zeros([2,self.data.shape[1]], dtype=float)\n",
    "        self.pX_0[1][self.data.shape[1]-1] = 1 \n",
    "        \n",
    "        \n",
    "        #P(X|C=0)\n",
    "        for j in range(self.data.shape[1]-1):  #we use -1 in order not to count the probability of the category row too\n",
    "            X1_C0 = 0\n",
    "            X0_C0 = 0\n",
    "            for i in range(self.data.shape[0]):\n",
    "                if self.data[i][self.data.shape[1]-1] != 0:\n",
    "                    continue\n",
    "                \n",
    "                if self.data[i][j] ==1:\n",
    "                    X1_C0+=1\n",
    "                else: \n",
    "                    X0_C0+=1\n",
    "                \n",
    "                    \n",
    "            self.pX_1[0][j]= (X1_C0+1)/(self.total_neg+2)\n",
    "            self.pX_0[0][j] = (X0_C0+1)/(self.total_neg+2)\n",
    "            #print(self.pX_1[0][j], self.pX_0[0][j])\n",
    "            \n",
    "            \n",
    "        #P(X|C=1)\n",
    "        for j in range(self.data.shape[1]-1):  #we use -1 in order not to count the probability of the category row too\n",
    "            X1_C1 = 0\n",
    "            X0_C1 = 0\n",
    "            for i in range(self.data.shape[0]):\n",
    "                if self.data[i][self.data.shape[1]-1] != 1:\n",
    "                    continue\n",
    "                \n",
    "                if self.data[i][j] ==1:\n",
    "                    X1_C1+=1\n",
    "                else: \n",
    "                    X0_C1+=1\n",
    "                    \n",
    "            self.pX_1[1][j]= (X1_C1+1)/(self.total_pos+2)\n",
    "            self.pX_0[1][j] = (X0_C1+1)/(self.total_pos+2)\n",
    "            #print(self.pX_1[1][j], self.pX_0[1][j])\n",
    "    \n",
    "        \n",
    "        \n",
    "        #print(\"Finished building the probability tables\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # def prob(self, pos,x,c):\n",
    "    \n",
    "    #     X_counter=0\n",
    "    #     C_counter=0\n",
    "    #     for i in range(self.vectors.shape[0]):\n",
    "    #         if self.vectors[i][self.vectors.shape[1]-1]!=c:\n",
    "    #             continue\n",
    "    #         C_counter+=1\n",
    "    #         if self.vectors[i][pos]==x:\n",
    "    #             X_counter+=1 \n",
    "                \n",
    "    #     return (X_counter+1)/(C_counter+2)   #using Laplace estimator\n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    def P1_X(self, vector):\n",
    "        p1= self.total_pos/(self.total_pos+self.total_neg)\n",
    "        \n",
    "        pX_C1 =1\n",
    "        for i, w in enumerate(vector):\n",
    "            if w==0:\n",
    "                pX_C1 = pX_C1*self.pX_0[1][i]\n",
    "            else: \n",
    "                pX_C1 = pX_C1*self.pX_1[1][i]\n",
    "                \n",
    "            \n",
    "            \n",
    "        return pX_C1*p1 \n",
    "\n",
    "\n",
    "\n",
    "    def P0_X(self, vector):\n",
    "        p0= self.total_neg/(self.total_pos+self.total_neg)\n",
    "        \n",
    "        pX_C0 =1\n",
    "        for i, w in enumerate(vector):\n",
    "            if w==0:\n",
    "                pX_C0 = pX_C0*self.pX_0[0][i]\n",
    "            else: \n",
    "                pX_C0 = pX_C0*self.pX_1[0][i]\n",
    "            \n",
    "            \n",
    "        return pX_C0*p0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "       \n",
    "    \n",
    "    # def P1_X(self, vector, total_pos, total_neg):\n",
    "    #     p1= total_pos/(total_pos+total_neg)\n",
    "        \n",
    "    #     pX_C1= 1\n",
    "        \n",
    "    #     for i, w in enumerate(vector):\n",
    "    #         #word= words_final[i]\n",
    "    #         if w==0:\n",
    "    #             pX_C1*= self.prob(i, 0, 1)\n",
    "    #             print(\"CALCULATING prob of X when C=1\")\n",
    "    #         else:\n",
    "    #             pX_C1*=self.prob(i, 1, 1)\n",
    "    #             print(\"CALCULATING prob of x=0 when c=0\")\n",
    "\n",
    "    #         if i==(len(vector)-1):\n",
    "    #             break\n",
    "    #     return pX_C1\n",
    "    \n",
    "    \n",
    "    \n",
    "    # def P0_X(self, vector, total_pos, total_neg):\n",
    "    #     p0= total_neg/(total_pos+total_neg)\n",
    "        \n",
    "    #     pX_C0= 1\n",
    "        \n",
    "    #     for i, w in enumerate(vector):\n",
    "    #         #word= words_final[i]\n",
    "    #         if w==0:\n",
    "    #             pX_C0*= self.prob(i, 0, 0)\n",
    "    #             print(\"CALCULATING prob of x=0 when c=0\")\n",
    "    #         else:\n",
    "    #             pX_C0*=self.prob(i, 1, 0)\n",
    "    #             print(\"CALCULATING prob of x=0 when c=0\")\n",
    "\n",
    "    #         if i==(len(vector)-1):\n",
    "    #             break\n",
    "    #     return pX_C0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classification_report(y_real, y_pred, p=1):\n",
    "    #print(y_pred)\n",
    "    reports= np.zeros([3,5])\n",
    "    \n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    false_neg = 0\n",
    "    support1= 0\n",
    "    support0= 0\n",
    "    \n",
    "    # corr_ident= 0\n",
    "    # incorr_ident= 0  \n",
    "    # total_neg= 0\n",
    "    for i, y in enumerate(y_pred):\n",
    "        if y== y_real[i] and y==1:\n",
    "            true_pos+=1\n",
    "        if y!= y_real[i] and y==1:\n",
    "            false_pos+=1\n",
    "        if y!= y_real[i] and y==0:\n",
    "            false_neg+=1\n",
    "        if y==y_real[i] and y==0:\n",
    "            true_neg+=1\n",
    "        if y_real[i]==1:\n",
    "            support1+=1\n",
    "            \n",
    "    if (true_pos+false_neg) != 0:\n",
    "        recall= true_pos/(true_pos+false_neg)\n",
    "    else:\n",
    "        recall=0\n",
    "    \n",
    "    if(true_pos+false_pos) !=0:\n",
    "        precision = true_pos/ (true_pos +false_pos)\n",
    "    else:\n",
    "        precision=0\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # print(recall)\n",
    "    # print(precision)\n",
    "    if (precision+recall) !=0 :\n",
    "        f1_score= 2*(precision*recall)/(precision+recall)\n",
    "    else:\n",
    "        f1_score=0\n",
    "        \n",
    "    accuracy = (true_pos + true_neg) / len(y_real)\n",
    "    \n",
    "    reports[1][0]= recall\n",
    "    reports[1][1]= precision\n",
    "    reports[1][2]= f1_score\n",
    "    \n",
    "    reports[1][4] = support1\n",
    "    \n",
    "    if(true_neg+false_pos)!=0:\n",
    "        #print('Percentage of negatives found correctly: ', true_neg/(true_neg+false_pos) )\n",
    "        \n",
    "        reports[0][3]= true_neg/(true_neg+false_pos)\n",
    "    \n",
    "    if (true_pos+false_neg) !=0:\n",
    "        #print('Percentage of positives found correctly: ', true_pos/(true_pos+false_neg))\n",
    "        reports[1][3]= true_pos/(true_pos+false_neg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # reports[2][0] = accuracy \n",
    "    # reports[2][1] = accuracy \n",
    "    # reports[2][2] = accuracy \n",
    "    # reports[2][3] = len(y_real) \n",
    "    \n",
    "    \n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    false_neg = 0\n",
    "    for i, y in enumerate(y_pred):\n",
    "        if y== y_real[i] and y==0:\n",
    "            true_pos+=1\n",
    "        if y!= y_real[i] and y==0:\n",
    "            false_pos+=1\n",
    "        if y!= y_real[i] and y==1:\n",
    "            false_neg+=1\n",
    "        if y==y_real[i] and y==1:\n",
    "            true_neg+=1\n",
    "        if y_real[i]==0:\n",
    "            support0+=1\n",
    "    if (true_pos+false_neg) != 0:\n",
    "        recall= true_pos/(true_pos+false_neg)\n",
    "    else:\n",
    "        recall=0\n",
    "    \n",
    "    if(true_pos+false_pos) !=0:\n",
    "        precision = true_pos/ (true_pos +false_pos)\n",
    "    else:\n",
    "        precision=0\n",
    "    \n",
    "    # print(recall)\n",
    "    # print(precision)\n",
    "    if (precision+recall) !=0 :\n",
    "        f1_score= 2*(precision*recall)/(precision+recall)\n",
    "    else:\n",
    "        f1_score=0\n",
    "    \n",
    "    reports[0][0]= recall\n",
    "    reports[0][1]= precision\n",
    "    reports[0][2]= f1_score\n",
    "    \n",
    "    reports[0][4] = support0\n",
    "    \n",
    "    \n",
    "    reports[2][0] = accuracy\n",
    "    reports[2][1]= accuracy\n",
    "    reports[2][2] = accuracy\n",
    "    reports[2][3] = accuracy\n",
    "    reports[2][4] = support0 + support1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # support_proportion0= support0/len(y_real)    \n",
    "    # support_proportion1= support1/len(y_real)\n",
    "    \n",
    "    \n",
    "    # macro_avg_p= (reports[0][1]+reports[1][1])/2\n",
    "    # macro_avg_r= (reports[0][0] + reports[1][0])/2\n",
    "    # macro_avg_f1 = (reports[0][2] + reports[1][2])/2\n",
    "    \n",
    "    # weighted_avg_r = (reports[0][0]*support_proportion1 + reports[1][0]*support_proportion0)\n",
    "    # weighted_avg_p= (reports[0][1]*support_proportion1 + reports[1][1]*support_proportion0)\n",
    "    # weighted_avg_f1= (reports[0][2]*support_proportion1 + reports[1][2]*support_proportion0)\n",
    "    \n",
    "    # reports[3][0] = macro_avg_r\n",
    "    # reports[3][1] = macro_avg_p\n",
    "    # reports[3][2] = macro_avg_f1\n",
    "    # reports[3][3] = len(y_real)\n",
    "    \n",
    "    # reports[4][0]= weighted_avg_r\n",
    "    # reports[4][1]= weighted_avg_p\n",
    "    # reports[4][2] = weighted_avg_f1\n",
    "    # reports[4][3] =len(y_real)\n",
    "    \n",
    "    \n",
    "    reports_df= pd.DataFrame(reports, columns=['Recall', 'Precision', 'F1-score', 'Accuracy', 'Support'], index=['0','1', 'general accuracy'])\n",
    "    \n",
    "    if p==1:\n",
    "        \n",
    "        print('positives that didn\\'t find: ', false_neg)\n",
    "        print('negatives that didn\\'t find: ', false_pos)\n",
    "        \n",
    "        display(reports_df)\n",
    "    \n",
    "    return reports\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT FUNCTION IN ORDER TO SPLIT DATA INTO X AND Y\n",
    "def split(vectors):\n",
    "    x = vectors[:,:-1]\n",
    "    y = vectors[:,-1]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naive_bayes:\n",
    "\n",
    "        \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.total_pos= 0\n",
    "        self.total_neg=0\n",
    "        self.data= np.zeros([x.shape[0],x.shape[1]+1], dtype=int)\n",
    "        for i in range(x.shape[0]):\n",
    "            \n",
    "            for j in range(x.shape[1]):\n",
    "                #if j== self.data.shape[1]:\n",
    "                 #   self.data[i][j]= y[i]\n",
    "                \n",
    "                self.data[i][j]= x[i][j]\n",
    "                \n",
    "            \n",
    "\n",
    "        for i in range(self.data.shape[0]):\n",
    "            self.data[i][self.data.shape[1]-1]= y[i]\n",
    "            if y[i]== 1:\n",
    "                self.total_pos+=1\n",
    "            else:\n",
    "                self.total_neg+=1\n",
    "                    \n",
    "        \n",
    "        #data_df= pd.DataFrame(self.data, columns=words_final+ ['Category'])\n",
    "        #display(data_df)\n",
    "        self.p= Probability(self.data, self.total_pos, self.total_neg)\n",
    "        self.p.prob_tables_training_data()\n",
    "        return self.data\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self, x_data):\n",
    "        y_data= [0]*x_data.shape[0]\n",
    "        #p=Probability(self.data)\n",
    "        #self.p.prob_tables_training_data()\n",
    "        #print(\"starting predictions\")\n",
    "        for i in range(x_data.shape[0]):\n",
    "            if self.p.P1_X(x_data[i]) > self.p.P0_X(x_data[i]):\n",
    "                y_data[i]= 1\n",
    "            else:\n",
    "                y_data[i]=0\n",
    "            \n",
    "        #print(\"finished predictions\")\n",
    "        return y_data\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IG(\"bad\",12500, 12500, '/Users/michail/Downloads/aclImdb/train/pos', '/Users/michail/Downloads/aclImdb/train/neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#TRAINING THE ALGORITHM\n",
    "\n",
    "#files_positive =Path('/Users/michail/Downloads/aclImdb/train/pos').glob('*.txt')\n",
    "#files_negative= Path('/Users/michail/Downloads/aclImdb/train/neg').glob('*.txt')\n",
    "\n",
    "path_pos= '/Users/michail/Downloads/aclImdb/train/pos'\n",
    "path_neg= '/Users/michail/Downloads/aclImdb/train/neg'\n",
    "\n",
    "t = tokenizer()\n",
    "\n",
    "train_vectors= t.tokenize(path_pos, path_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pos= '/Users/michail/Downloads/aclImdb/train/pos'\n",
    "path_neg= '/Users/michail/Downloads/aclImdb/train/neg'\n",
    "\n",
    "test_vectors, df_study= t.tokenize_new(path_pos, path_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pos= '/Users/michail/Downloads/aclImdb/test/pos'\n",
    "path_neg= '/Users/michail/Downloads/aclImdb/test/neg'\n",
    "\n",
    "test_vectors, df_study= t.tokenize_new(path_pos, path_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_study.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train= split(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb= naive_bayes()\n",
    "nb.fit(x_train, y_train)\n",
    "print('fit done')\n",
    "y_pred = nb.predict(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test= split(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#def plot_learning_curve(x_train, y_test):\n",
    "    \n",
    "    \n",
    "plt.figure()\n",
    "plt.title(\"Training curve\")\n",
    "plt.xlabel(\"Examples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "    \n",
    "tr_samples= np.linspace(1, x_train.shape[0],100)\n",
    "    \n",
    "eval=[]\n",
    "for sample in tr_samples:\n",
    "    rep_train = classification_report(y_train[:int(sample)], nb.predict(x_train[:int(sample)]), 0)\n",
    "\n",
    "    results={\n",
    "        'sample number': sample,\n",
    "        'recall': rep_train[0][0],\n",
    "        'precision': rep_train[0][1],\n",
    "        'f1-score': rep_train[0][2],\n",
    "        'general accuracy': rep_train[2][0]\n",
    "    }\n",
    "    eval.append(results)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_recall= [s['recall'] for s in eval]\n",
    "train_precision= [s['precision'] for s in eval]\n",
    "train_accuracy = [s['general accuracy'] for s in eval]\n",
    "\n",
    "train_sample_numbers= [s['sample number'] for s in eval]\n",
    "\n",
    "plt.plot(train_sample_numbers,train_precision , 'r', label= \"Training score\")    \n",
    "    \n",
    "print(\"\\nTraining sample size: \", train_sample_numbers )\n",
    "    \n",
    "test_samples= np.linspace(1, x_test.shape[0], 100)\n",
    "eval2=[]\n",
    "for sample in test_samples:\n",
    "    rep_test= classification_report(y_test[:int(sample)], nb.predict(x_test[:int(sample)]), 0)\n",
    "    \n",
    "    results2={\n",
    "        'sample number': sample,\n",
    "        'recall': rep_test[0][0],\n",
    "        'precision': rep_test[0][1],\n",
    "        'f1-score': rep_test[0][2],\n",
    "        'general accuracy': rep_test[2][0]\n",
    "    }\n",
    "    \n",
    "    eval2.append(results2)\n",
    "\n",
    "\n",
    "test_recall= [s['recall'] for s in eval2]\n",
    "test_precision= [s['precision'] for s in eval2]\n",
    "test_sample_numbers= [s['sample number'] for s in eval2]\n",
    "test_accuracy = [s['general accuracy'] for s in eval2]\n",
    "    \n",
    "plt.plot(test_sample_numbers,test_precision , 'g', label= \"Test score\")    \n",
    "    \n",
    "\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(x_train, y_train)\n",
    "print(classification_report(y_train, nb.predict(x_train),\n",
    "                            zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, nb.predict(x_test),\n",
    "                            zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nb = RandomForestClassifier()\n",
    "nb.fit(x_train, y_train)\n",
    "print(classification_report(y_train, nb.predict(x_train),\n",
    "                            zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, nb.predict(x_test),\n",
    "                            zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
